<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Geometric Factor Analysis</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/js/all.min.js"></script>
    <!-- Add MathJax Support -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$','$'], ['\\(','\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']],
                processEscapes: true
            }
        });
    </script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }

        :root {
            --primary-color: #4a90e2;
            --secondary-color: #2c3e50;
            --accent-color: #e74c3c;
            --background-color: #f5f6fa;
            --text-color: #2d3436;
        }

        body {
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background-color);
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        .nav {
            background: white;
            padding: 20px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .nav ul {
            list-style: none;
            display: flex;
            justify-content: center;
            gap: 30px;
            align-items: center;
        }

        .nav a {
            text-decoration: none;
            color: var(--secondary-color);
            font-weight: 500;
            transition: all 0.3s ease;
            padding: 8px 16px;
            border-radius: 4px;
        }

        .nav a:hover {
            background-color: var(--primary-color);
            color: white;
        }

        .social-icons a {
            font-size: 1.5em;
            margin: 0 10px;
        }

        .section {
            margin: 60px 0;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .section h1 {
            font-size: 2.5em;
            margin-bottom: 30px;
            color: var(--secondary-color);
            text-align: center;
            position: relative;
            padding-bottom: 15px;
        }

        .section h1::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 50%;
            transform: translateX(-50%);
            width: 100px;
            height: 4px;
            background: var(--primary-color);
            border-radius: 2px;
        }

        .subsection {
            margin: 40px 0;
            padding: 20px;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .section p, .subsection p {
            font-size: 1.2em;  /* Increase as needed: 1.2em, 1.3em etc */
            line-height: 1.8;  /* Improves readability */
        }

        .subsection h2 {
            font-size: 1.8em;
            margin-bottom: 20px;
            color: var(--secondary-color);
            border-left: 4px solid var(--primary-color);
            padding-left: 15px;
        }

        .figure-container {
            margin: 30px 0;
            text-align: center;
        }

        .figure-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }

        .figure-caption {
            margin-top: 15px;
            font-style: italic;
            color: var(--secondary-color);
            font-size: 0.9em;
        }
        .equation-container {
            margin: 2em 0;
            text-align: center;
            overflow-x: auto;
            padding: 1em;
        }

        footer {
            background: var(--secondary-color);
            color: white;
            text-align: center;
            padding: 30px;
            margin-top: 60px;
        }

        @media (max-width: 768px) {
            .nav ul {
                flex-wrap: wrap;
                gap: 15px;
            }
            
            .section h1 {
                font-size: 2em;
            }
            
            .subsection h2 {
                font-size: 1.5em;
            }
        }
    </style>
</head>
<body>
    <nav class="nav">
        <ul>
            <li><a href="/">Home</a></li>
            <li><a href="#about">About</a></li>
            <li><a href="#research">Research</a></li>
            <li><a href="#projects">Projects</a></li>
            <li><a href="#blog">Blog</a></li>
            <li><a href="/cv.pdf">CV</a></li>
            <li class="social-icons">
                <a href="https://github.com/yourusername" target="_blank"><i class="fab fa-github"></i></a>
                <a href="https://linkedin.com/in/yourusername" target="_blank"><i class="fab fa-linkedin"></i></a>
            </li>
        </ul>
    </nav>

    <div class="container">
        <section class="section">
            <h1>Geometric Factor Analysis</h1>
            <p>Exploring the intersection of geometry and statistical analysis to uncover hidden patterns in high-dimensional data structures.</p>
            <div class="figure-container">
                <img src="images/gfa/PCvVarimaxvOurs.png" alt="GFA Overview">
                <p class="figure-caption">Figure 1. Visualization of geometric factor analysis compared to other classical techniques.</p>
            </div>
        </section>
        
        <div class="subsection">
            <h2>Motivation: A problem in Psychometry</h2>
            <p>Ever wondered how we can sort people into different personality groups? Here's the puzzle researchers need to solve: Starting with all the survey answers (imagine a big table of numbers), they need to figure out which personality type best fits each person. It's like having a basket of mixed fruits and trying to sort them into different groups - but instead of looking at color and shape, we're working with numbers that represent how people answered questions about themselves.</p>
            <div class="figure-container">
                <img src="/api/placeholder/600/300" alt="Methodology Diagram">
                <p class="figure-caption">Figure 2. Methodological framework showing the integration of geometric principles.</p>
            </div>
            <p>The <span style="font-weight: bold;">mathematical challenge</span> is straightforward: we have all the survey answers from many people (<span style="color: darkorange;">data points</span> collected in a matrix <span style="color: darkorange;">$X\in\mathbb{R}^{d\times k}$</span>), and we need to determine which <span style="color: #FF1493;">personality type</span> corresponds to each individual (recover the <span style="color: #FF1493;">labels $y\in\{1,\ldots,k\}$</span>). This task is usually formulated through a linear model:</p>
            $$X = FW^\top$$
            <p>where <span style="color: #FF1493;">$F$</span> represents the <span style="color: #FF1493;">latent factors (personality types)</span> and $W$ denotes the factor loadings, which capture how each dimension contributes to the personality types.</p>

            <div style="display: flex; gap: 20px; align-items: center; margin: 20px 0;">
                <!-- Left column - Image -->
                <div style="flex: 1;">
                    <img src="path_to_your_image.png" alt="Radial Structure Visualization" style="width: 100%; max-width: 400px;">
                </div>
                
                <!-- Right column - Text -->
                <div style="flex: 1;">
                    <p>Remarkably, in psychometrics, the data often exhibits <span style="font-weight: bold; font-style: italic;">radial structure</span>. Intuitively, this means that the factorization looks like</p>
                    
                    <p>$$X_i = Fw_i = [F_1, F_2, \ldots, F_k] \begin{pmatrix} 1 \\ \varepsilon_1 \\ \vdots \\ \varepsilon_{k-1} \end{pmatrix}$$</p>
            
                    <p>and most information can be captured by <span style="color: #FF1493; font-weight: bold;">one factor</span>!</p>
                </div>
            </div>

<p>Let's tackle the key challenge: <span style="color: #8A2BE2; font-weight: bold;">how can we uncover the hidden radial structure in our data?</span> Your first instinct might be to reach for <span style="font-weight: bold;">principal components analysis (PCA)</span> - it's a tried and true method for finding important directions in data. But there's a catch when dealing with radial structures. </p>
    
<div style="display: flex; gap: 20px; align-items: center; margin: 20px 0;">
    <!-- Left column - Text -->
    <div style="flex: 1;">
        <p>Think about what happens with radial data: imagine points spreading out like spokes of a wheel. In this case, something interesting occurs - the <span style="color: red">covariance matrix ends up looking almost like a scaled identity matrix</span>:</p>
 
        $$\mathbb{E}\left[XX^\top\right] \approx \lambda Id$$
 
        <p>This presents a real challenge. Why? Because when your covariance matrix is nearly proportional to the identity, the principal components we get are essentially <span style="color: red;">pointing in random directions</span> - not very helpful for understanding the true structure of our data!</p>
    </div>
    
    <!-- Right column - Image -->
    <div style="flex: 1;">
        <img src="path_to_your_image.png" alt="Radial Structure PCA Issues" style="width: 100%; max-width: 400px;">
    </div>
 </div>
 
<p>One classical approach to handling radial structure is the <span style="color: #8A2BE2; font-weight: bold;">Varimax rotation method</span>. This technique works by taking the orthogonal basis that PCA finds and applying a clever rotation to better understand how the <span style="color: #FF1493;">factors</span> relate to each other.</p>

<p>The mathematical machinery behind Varimax is sophisticated but purposeful. <span style="color: #008080; font-weight: bold;">To find the optimal rotation</span>, it maximizes a specific objective function that involves fourth-order moments of the data:</p>

$$R^\star \in \text{argmax}_{R\in O(k)}\sum_{j=1}^k \frac{1}{d}\sum_{i=1}^d\left([\Lambda R]_{ij}^4 - \left(\frac{1}{d}\sum_{l=1}^d[\Lambda R]_{li}^2\right)^2\right)$$

<p><span style="color: #008080;">where $\Lambda = \text{PC}$.</span><span style="color: #800000; font-weight: bold;">The intuition</span> behind this approach is that by fitting to the fourth moment, we can capture more complex relationships in the data than traditional PCA, which only considers second moments.</p>

<p>But <span style="color: #8A2BE2; font-weight: bold;">how well does Varimax actually perform?</span> Recent theoretical work by <span style="color: #20B2AA; font-weight: bold;">Rohe (2022)</span> provides some answers. Under <span style="text-decoration: underline;">several</span> assumptions about how the <span style="color: #FF1493;">factors</span> are distributed, we can bound the error of Varimax's estimates. Specifically, the distance between the estimated <span style="color: #FF1493;">factor model</span> $\hat{X}$ and the <span style="color: #800000;">true factors (up to signed permutations)</span> $XP_n$ is bounded by $O_p(\Delta_n^{-0.24}\log^{3.75} n)$ where this bound involves the mean of the data scaled by the sample size. While this theoretical guarantee might sound promising, practical applications often reveal limitations of the method.</p>

<p>Are there <span style="color: #8A2BE2; font-weight: bold;">better alternatives?</span> One promising approach is <span style="color: #20B2AA; font-weight: bold;">subspace clustering</span>, which aims to recover a <span style="font-weight: bold;">union of subspaces</span> that capture <span style="font-weight: bold;">most of the data's information</span>. Our problem fits naturally into this framework as a special case, where we're looking for $k$ different 1-dimensional subspaces that represent our <span style="color: #FF1493;">factors</span>. This approach has proven particularly valuable in computer vision applications, as demonstrated by Vidal's work in 2011.</p>

<p>However, <span style="color: #8A2BE2; font-weight: bold;">subspace clustering comes with several significant challenges</span>. First and foremost, most existing methods <span style="color: #8B0000;">lack robustness</span>. Even the most sophisticated solution to date - the <span style="color: #20B2AA; font-weight: bold">Robust subspace clustering</span> approach developed by Soltanolkotabi in 2014 - has its drawbacks. It requires <span style="color: #8B0000;">constructing an $n^2$ similarity matrix</span> and then <span style="color: #8B0000;">extracting the top-$k$ eigenvalues</span> in a second stage, making it computationally intensive. Moreover, since the subspaces aren't required to be orthogonal to each other, this approach might allow <span style="color: #FF1493;">factors to correlate</span>, which isn't always desirable.</p>

<p>This brings us to the question of the day: <span style="color: #20B2AA;">Can we develop a method that's both scalable and statistically robust?</span> The answer is <span style="font-weight: bold;">yes!</span> This is where we introduce <span style="color: #00008B; font-weight: bold;">Geometric Factor Analysis (GFA)</span>, a novel approach that addresses these limitations.</p>
            
        </div>

        <div class="subsection">
            <h2>Project Overview</h2>
            <p><span style="color: #20B2AA; font-weight: bold;">Idea:</span> The data is close to being <span style="font-weight: bold;">supported on a cross</span></p>

            <p><span style="color: #20B2AA;">Let's find the closest distribution with that property!</span></p>

            <p>Considering<br>
            $\mathscr{P}_\times = \{\text{probability distributions supported on a cross with }k\text{ factors}\}$,<br>
            we pursue solving</p>

            <p>$\min_{\nu\in\mathscr{P}_\times} W_2^2(\mu,\nu)$</p>

            <p>In principle, this is an <span style="font-weight: bold;">infinite-dimensional</span> problem,<br>
            BUT...</p>
            <div class="figure-container">
                <img src="/api/placeholder/600/300" alt="Methodology Diagram">
                <p class="figure-caption">Figure 2. Methodological framework showing the integration of geometric principles.</p>
            </div>
            <p><span style="color: #8A2BE2; font-size: larger; font-weight: bold;">Reduction to finite dimension</span></p>

            <p>Let's consider instead<br>
            $St(d,k) := \{U \in \mathbb{R}^{d\times k}: U^\top U = I_{k\times k}\}$ and equivalently<br>
            formulate</p>

            <p>$\min_{U\in St(d,k)} W_2^2(\mu, P_U\#\mu)$</p>

            <p>As we've seen, in practice, we are given $x_1,\ldots,x_n \sim \mu$<br>
            i.i.d., so... we aim to solve</p>

            <p><span style="border: 2px solid #20B2AA; padding: 5px;">$\min_{U\in St(d,k)} \sum_{i=1}^n \|X_i - \Pi_{X_U}(X_i)\|^2$</span></p>

            <p><span style="color: #20B2AA;">↪ Optimize on a manifold?</span></p>

        </div>

        <div class="subsection">
            <h2>Methodology</h2>
            <p>We develop a comprehensive framework that combines differential geometry with statistical learning theory. Our approach incorporates manifold learning techniques while preserving the intrinsic geometric structure of the data.</p>
        </div>

        <div class="subsection">
            <h2>Results</h2>
            <p>Our experiments demonstrate significant improvements in both accuracy and interpretability. The geometric constraints provide natural regularization, leading to more stable and meaningful factor solutions.</p>
            
            <div class="figure-container">
                <img src="/api/placeholder/700/350" alt="Results Comparison">
                <p class="figure-caption">Figure 3. Comparative analysis of traditional vs. geometric factor analysis results.</p>
            </div>
        </div>

        <div class="subsection">
            <h2>Related Publications</h2>
            <p>Smith, J., et al. (2024). "Geometric Factor Analysis: A Novel Approach to Understanding High-Dimensional Data Structures." Journal of Statistical Computing.<br><br>
            Johnson, M., et al. (2023). "Applications of Geometric Factor Analysis in Biological Systems." Proceedings of the International Conference on Machine Learning.</p>
        </div>
    </div>

    <footer>
        <p>&copy; 2024 Daniel López-Castaño. All rights reserved.</p>
    </footer>
</body>
</html>