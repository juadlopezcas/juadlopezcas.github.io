<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Geometric Factor Analysis</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/js/all.min.js"></script>
    <!-- Add MathJax Support -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$','$'], ['\\(','\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']],
                processEscapes: true
            }
        });
    </script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }

        :root {
            --primary-color: #4a90e2;
            --secondary-color: #2c3e50;
            --accent-color: #e74c3c;
            --background-color: #f5f6fa;
            --text-color: #2d3436;
        }

        body {
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background-color);
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        .nav {
            background: white;
            padding: 20px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .nav ul {
            list-style: none;
            display: flex;
            justify-content: center;
            gap: 30px;
            align-items: center;
        }

        .nav a {
            text-decoration: none;
            color: var(--secondary-color);
            font-weight: 500;
            transition: all 0.3s ease;
            padding: 8px 16px;
            border-radius: 4px;
        }

        .nav a:hover {
            background-color: var(--primary-color);
            color: white;
        }

        .social-icons a {
            font-size: 1.5em;
            margin: 0 10px;
        }

        .section {
            margin: 60px 0;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .section h1 {
            font-size: 2.5em;
            margin-bottom: 30px;
            color: var(--secondary-color);
            text-align: center;
            position: relative;
            padding-bottom: 15px;
        }

        .section h1::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 50%;
            transform: translateX(-50%);
            width: 100px;
            height: 4px;
            background: var(--primary-color);
            border-radius: 2px;
        }

        .subsection {
            margin: 40px 0;
            padding: 20px;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .section p, .subsection p {
            font-size: 1.2em;  /* Increase as needed: 1.2em, 1.3em etc */
            line-height: 1.8;  /* Improves readability */
        }

        .subsection h2 {
            font-size: 1.8em;
            margin-bottom: 20px;
            color: var(--secondary-color);
            border-left: 4px solid var(--primary-color);
            padding-left: 15px;
        }

        .figure-container {
            margin: 30px 0;
            text-align: center;
        }

        .figure-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }

        .figure-caption {
            margin-top: 15px;
            font-style: italic;
            color: var(--secondary-color);
            font-size: 0.9em;
        }
        .equation-container {
            margin: 2em 0;
            text-align: center;
            overflow-x: auto;
            padding: 1em;
        }

        footer {
            background: var(--secondary-color);
            color: white;
            text-align: center;
            padding: 30px;
            margin-top: 60px;
        }

        @media (max-width: 768px) {
            .nav ul {
                flex-wrap: wrap;
                gap: 15px;
            }
            
            .section h1 {
                font-size: 2em;
            }
            
            .subsection h2 {
                font-size: 1.5em;
            }
        }
    </style>
</head>
<body>
    <nav class="nav">
        <ul>
            <li><a href="/">Home</a></li>
            <li><a href="#about">About</a></li>
            <li><a href="#research">Research</a></li>
            <li><a href="#projects">Projects</a></li>
            <li><a href="#blog">Blog</a></li>
            <li><a href="/cv.pdf">CV</a></li>
            <li class="social-icons">
                <a href="https://github.com/yourusername" target="_blank"><i class="fab fa-github"></i></a>
                <a href="https://linkedin.com/in/yourusername" target="_blank"><i class="fab fa-linkedin"></i></a>
            </li>
        </ul>
    </nav>

    <div class="container">
        <section class="section">
            <h1>Geometric Factor Analysis</h1>
            <p>Exploring the intersection of geometry and statistical analysis to uncover hidden patterns in high-dimensional data structures.</p>
        </section>
        
        <div class="subsection">
            <h2>Motivation: A problem in Psychometry</h2>
            <p>Ever wondered how we can sort people into different personality groups? Here's the puzzle researchers need to solve: Starting with all the survey answers (imagine a big table of numbers), they need to figure out which personality type best fits each person. It's like having a basket of mixed fruits and trying to sort them into different groups - but instead of looking at color and shape, we're working with numbers that represent how people answered questions about themselves.</p>
            <div class="figure-container" style="margin: 20px 0; text-align: center;">
                <!-- Add error handling to debug image loading -->
                <img 
                    src="images/survey.png" 
                    alt="Survey Diagram" 
                    style="max-width: 100%; height: auto;"
                    onerror="console.log('Error loading image'); this.style.border='1px solid red';"
                >
                <p class="figure-caption" style="font-style: italic; margin-top: 10px;">
                    Figure 2. Methodological framework showing the integration of geometric principles.
                </p>
            </div>
            <p>The <span style="font-weight: bold;">mathematical challenge</span> is straightforward: we have all the survey answers from many people (<span style="color: darkorange;">data points</span> collected in a matrix <span style="color: darkorange;">$X\in\mathbb{R}^{d\times k}$</span>), and we need to determine which <span style="color: #FF1493;">personality type</span> corresponds to each individual (recover the <span style="color: #FF1493;">labels $y\in\{1,\ldots,k\}$</span>). This task is usually formulated through a linear model:</p>
            $$X = FW^\top$$
            <p>where <span style="color: #FF1493;">$F$</span> represents the <span style="color: #FF1493;">latent factors (personality types)</span> and $W$ denotes the factor loadings, which capture how each dimension contributes to the personality types.</p>

            <div style="display: flex; gap: 20px; align-items: center; margin: 20px 0;">
                <!-- Left column - Image -->
                <div style="flex: 1; display: flex; justify-content: center;">
                    <img src="images/scatter.png" alt="Radial Structure Visualization" style="width: 100%; max-width: 400px; align-items: center">
                </div>
                
                <!-- Right column - Text -->
                <div style="flex: 1;">
                    <p>Remarkably, in psychometrics, the data often exhibits <span style="font-weight: bold; font-style: italic;">radial structure</span>. Intuitively, this means that the factorization looks like</p>
                    
                    <p>$$X_i = Fw_i = [F_1, F_2, \ldots, F_k] \begin{pmatrix} 1 \\ \varepsilon_1 \\ \vdots \\ \varepsilon_{k-1} \end{pmatrix}$$</p>
            
                    <p>and most information can be captured by <span style="color: #FF1493; font-weight: bold;">one factor</span>!</p>
                </div>
            </div>

<p>Let's tackle the key challenge: <span style="color: #8A2BE2; font-weight: bold;">how can we uncover the hidden radial structure in our data?</span> Your first instinct might be to reach for <span style="font-weight: bold;">principal components analysis (PCA)</span> - it's a tried and true method for finding important directions in data. But there's a catch when dealing with radial structures. </p>
    
<div style="display: flex; gap: 20px; align-items: center; margin: 20px 0;">
    <!-- Left column - Text -->
    <div style="flex: 1;">
        <p>Think about what happens with radial data: imagine points spreading out like spokes of a wheel. In this case, something interesting occurs - the <span style="color: red">covariance matrix ends up looking almost like a scaled identity matrix</span>:</p>

        $$\mathbb{E}\left[XX^\top\right] \approx \lambda Id$$

        <p>This presents a real challenge. Why? Because when your covariance matrix is nearly proportional to the identity, the principal components we get are essentially <span style="color: red;">pointing in random directions</span> - not very helpful for understanding the true structure of our data!</p>
    </div>
    
    <!-- Right column - Image -->
    <div style="flex: 1; display: flex; justify-content: center;">
        <img src="images/PCA_Issues.png" alt="Radial Structure PCA Performance" style="width: 100%; max-width: 400px;">
    </div>
</div>

<p>One classical approach to handling radial structure is the <span style="color: #8A2BE2; font-weight: bold;">Varimax rotation method</span>. This technique works by taking the orthogonal basis that PCA finds and applying a clever rotation to better understand how the <span style="color: #FF1493;">factors</span> relate to each other.</p>

<p>The mathematical machinery behind Varimax is sophisticated but purposeful. <span style="color: #008080; font-weight: bold;">To find the optimal rotation</span>, it maximizes a specific objective function that involves fourth-order moments of the data:</p>

$$R^\star \in \text{argmax}_{R\in O(k)}\sum_{j=1}^k \frac{1}{d}\sum_{i=1}^d\left([\Lambda R]_{ij}^4 - \left(\frac{1}{d}\sum_{l=1}^d[\Lambda R]_{li}^2\right)^2\right)$$

<p><span style="color: #008080;">where $\Lambda$ corresponds to the principal components.</span><span style="color: #800000; font-weight: bold;"> The intuition</span> behind this approach is that by fitting to the fourth moment, we can capture more complex relationships in the data than traditional PCA, which only considers second moments.</p>

<div style="display: flex; gap: 20px; align-items: center; margin: 20px 0;">
    <!-- Left column - Image -->
    <div style="flex: 1; display: flex; justify-content: center;">
        <img src="images/varimax_issues.png" alt="Varimax Performance" style="width: 100%; max-width: 400px;">
    </div>
    
    <!-- Right column - Text -->
    <div style="flex: 1;">
        <p>But <span style="color: #8A2BE2; font-weight: bold;">how well does Varimax actually perform?</span> Recent theoretical work by <span style="color: #20B2AA; font-weight: bold;">Rohe (2022)</span> provides some answers. Under <span style="text-decoration: underline;">several</span> assumptions about how the <span style="color: #FF1493;">factors</span> are distributed, we can bound the error of Varimax's estimates. Specifically, the distance between the estimated <span style="color: #FF1493;">factor model</span> $\hat{X}$ and the <span style="color: #800000;">true factors (up to signed permutations)</span> $XP_n$ is bounded by $O_p(\Delta_n^{-0.24}\log^{3.75} n)$ where this bound involves the mean of the data scaled by the sample size. While this theoretical guarantee might sound promising, practical applications often reveal limitations of the method, as we see in the plot of th left.</p>
    </div>
</div>

<p>Are there <span style="color: #8A2BE2; font-weight: bold;">better alternatives?</span> One promising approach is <span style="color: #20B2AA; font-weight: bold;">subspace clustering</span>, which aims to recover a <span style="font-weight: bold;">union of subspaces</span> that capture <span style="font-weight: bold;">most of the data's information</span>. Our problem fits naturally into this framework as a special case, where we're looking for $k$ different 1-dimensional subspaces that represent our <span style="color: #FF1493;">factors</span>. This approach has proven particularly valuable in computer vision applications, as demonstrated by Vidal's work in 2011.</p>
<div class="figure-container">
    <img src="images/subspaces.png" alt="Subspace Clustering" style="width: 100%; max-width: 400px">
    <p class="figure-caption">Subspace clustering aims to reveal the inherent structure of data located in the union of subspaces.</p>
</div>  
<p>However, <span style="color: #8A2BE2; font-weight: bold;">subspace clustering comes with several significant challenges</span>. First and foremost, most existing methods <span style="color: #8B0000;">lack robustness</span>. Even the most known (or used) solution to date - the <span style="color: #20B2AA; font-weight: bold">Robust subspace clustering</span> approach developed by Soltanolkotabi in 2014 - has its drawbacks. It requires <span style="color: #8B0000;">constructing an $n^2$ similarity matrix</span> and then <span style="color: #8B0000;">extracting the top-$k$ eigenvalues</span> in a second stage, making it computationally intensive. Moreover, since the subspaces aren't required to be orthogonal to each other, this approach might allow <span style="color: #FF1493;">factors to correlate</span>, which isn't always desirable.</p>

<p>This brings us to the question of the day: <span style="color: #20B2AA;">Can we develop a method that's both scalable and statistically robust?</span> The answer is <span style="font-weight: bold;">yes!</span> This is where we introduce <span style="color: #00008B; font-weight: bold;">Geometric Factor Analysis (GFA)</span>, a novel approach that addresses these limitations.</p>
<div class="figure-container">
    <img src="images/PCvVarimaxvOurs.png" alt="Geometric Factor Analysis">
</div>            

        </div>

        <div class="subsection">
            <h2>Method</h2>

            <p><span style="color: #20B2AA; font-weight: bold;">Our key idea</span> stems from a fundamental observation: the data appears to be <span style="font-weight: bold;">supported on a cross</span>. Given this insight, let's pursue an elegant solution - finding the distribution that best matches this property.</p>
            
            <p>To formalize this approach, we consider the set</p> 
            
            $$\mathscr{P}_\times = \{\text{probability distributions supported on a cross with }k\text{ factors}\}$$ 
            
            <p>Our objective then becomes solving the optimization problem:</p>
            
            $$\min_{\nu\in\mathscr{P}_\times} W_2^2(\mu,\nu)$$
            
            <p>While this formulation initially presents itself as an <span style="font-weight: bold;">infinite-dimensional</span> problem, we can cleverly reduce it to something more manageable.</p>
            
            <p><span style="color: #8A2BE2; font-weight: bold;">A clever reduction to finite dimension</span> can be by reformulating our problem using the Stiefel manifold $St(d,k) := \{U \in \mathbb{R}^{d\times k}: U^\top U = I_{k\times k}\}$. This is by expressing our objective equivalently as:</p>
            
            $$\min_{U\in St(d,k)} W_2^2(\mu, P_U\#\mu)$$
            
            <p>In practice, since we work with finite samples $x_1,\ldots,x_n$ drawn independently from distribution $\mu$, our problem takes the concrete form:</p>
            
            $$\min_{U\in St(d,k)} \sum_{i=1}^n \|X_i - \Pi_{X_U}(X_i)\|^2$$
            
            <p>This formulation naturally leads us to an interesting question: <span style="color: #20B2AA;">how do we effectively optimize over this manifold?</span></p>
                
            <div class="figure-container">
                <img src="images/WFA1_axis_sum.png" alt="Methodology">
            </div>
        </div>
        
        <div class="subsection">
        <h2>Optimization Guarantees</h2>
            <p><span style="color: #20B2AA; font-size: larger;">Assumption:</span></p>
            <p>Let's start with a precise technical setup: we consider $\mu \in \mathscr{P}_{2,ac}(\mathbb{R}^d)$ as a convolved measure $\mu = \nu \star \tau\Phi$, 
                where $\nu,\Phi$ belong to $\mathscr{P}_2(\mathbb{R}^d)$, $\nu$ is supported on the cross $\bigcup_{i=1}^{k}\text{span}\{U_{i}^{*}\}$ for some $U^* \in St(d, k)$, and $\Phi$ is isotropic.</p>
            <div style="border: 2px solid #20B2AA; padding: 15px; margin: 15px 0;">
                <p><span style="color: #20B2AA; font-size: larger;">Theorem:</span></p>
                <p>Under these conditions, we get three guarantees:</p>
                <p>(i) The loss function $\mathscr{E}(U) = W_2^2(\mu, P_U\#\mu)$ is smooth,</p>
                <p>(ii) $U^*$ is a local minimizer of $\mathscr{E}(U)$, and</p>
                <p>(iii) in a neighborhood $N \subseteq St(d, k)$ around $U^*$, $\mathscr{E}(U)$ is strongly convex!</p>
            </div>
            <p>The story gets even more interesting when we look at the Hessian:</p>
            <div style="display: flex; align-items: center; justify-content: center; gap: 20px; margin: 20px 0;">
                <p>$L \text{ Id} \succeq \text{Hess}(\mathscr{E}(U)) \succeq m \text{ Id} \text{ in } N$</p>
            </div>
            <p>Because the Riemannian Hessian is bounded both above and below in the positive semidefinite order, we get two crucial properties. First, we have geodesic strong convexity (from the lower bound), and second, we have $L$-Lipschitz Riemannian gradients (from the upper bound). Together, these properties guarantee <span style="color: #20B2AA; font-weight: bold;">linear convergence</span> when we work in a geodesically convex chart where geodesic strong convexity holds.</p>
            <p>What do these theoretical guarantees mean in practice? </p>
            <div style="display: flex; gap: 20px; align-items: center; margin: 20px 0;">
                <!-- Left side - Plot -->
                <div style="flex: 1; display: flex; justify-content: center;">
                    <img src="images/iterations.png" alt="Performance Visualization" style="width: 100%; max-width: 400px;">
                </div>
                <!-- Right side - Text -->
                <div style="flex: 1;">
                    <p>First, we see a <span style="color: #32CD32; font-weight: bold;">rapid initial decay</span> - the algorithm quickly reduces errors near its starting point. Second, and perhaps more impressively, this performance stays <span style="color: #32CD32; font-weight: bold;">consistent</span> regardless of how many data points we're working with.</p>
                </div>
            </div>
            <div style="display: flex; gap: 20px; align-items: center; margin: 20px 0;">
                <!-- Left side - Text -->
                <div style="flex: 1;">
                    <p>The real advantage becomes clear when we look at computational efficiency. Our method outperforms <span style="color: #20B2AA; font-weight: bold;">robust subspace clustering</span> significantly. As we scale up to higher dimensions, our algorithm maintains a gentle, <span style="font-weight: bold;">linear increase</span> in computation time, while the traditional approach struggles with an <span style="color: #8B0000; font-weight: bold;">exponential growth</span> in processing time. This makes our method particularly valuable for handling high-dimensional data efficiently.</p>
                </div>
                <!-- Right side - Plots -->
                <div style="flex: 1; display: flex; flex-direction: column; gap: 10px;">
                    <div style="display: flex; justify-content: center;">
                        <img src="images/computation_time1.png" alt="Computation Time Plot 1" style="width: 100%; max-width: 400px;">
                    </div>
                    <div style="display: flex; justify-content: center;">
                        <img src="images/computation_time2.png" alt="Computation Time Plot 2" style="width: 100%; max-width: 400px;">
                    </div>
                </div>
            </div>
        </div>
        
        <div class="subsection">
            <h2>Statistical Guarantees</h2>
            <div style="display: flex; gap: 20px; align-items: center; margin: 20px 0;">
                <!-- Left column - Image -->
                <div style="flex: 1; display: flex; justify-content: center;">
                    <img src="images/wasserstein-estimation.png" alt="Wasserstein Distance Estimation" style="width: 100%; max-width: 400px;">
                </div>
                <!-- Right column - Text -->
            <div style="flex: 1;">
                <p>Let's tackle a fundamental question: How can we <span style="color: #FF1493; font-weight: bold;">estimate</span> the Wasserstein distance between our original distribution and its projection using only <span style="color: #FF1493;">data</span>?</p>

                <p>We start with samples:<br>
                $x_1, \ldots, x_n \sim \mu$ <span style="margin-left: 20px;">i.i.d. samples</span><br>
                $y_1, \ldots, y_n \sim P_{U^*}\#\mu$</p>

                <p>The key <span style="color: #20B2AA;">question</span> becomes: how many samples do we need?</p>
            </div>
            </div>
            <p>To understand this better, let's look at how we construct our empirical measures:</p>
            <div style="text-align: center; margin-bottom: 20px;">
                <p>$x_1, \ldots, x_n \sim \mu \qquad \longrightarrow \qquad \mu_n = \frac{1}{n}\sum_i \delta_{x_i}$</p>
                <p>$y_1, \ldots, y_n \sim \nu \qquad \longrightarrow \qquad \nu_n = \frac{1}{n}\sum_i \delta_{y_i}$</p>
            </div>
            <p>This leads to some questions: Can we trust that $W_2(\mu_n, \nu_n)$ approximates $W_2(\mu, \nu)$? Unfortunately, the answer is generally <span style="font-weight: bold;">no</span>. When we try to estimate $\mu$ through $W_2(\mu, \mu_n)$, we run into a serious <span style="color: red; font-weight: bold;">issue</span>: for measures absolutely continuous with respect to Lebesgue measure, we get the discouraging bound $W_2(\mu, \mu_n) \gtrsim n^{-1/d}$ almost surely!</p>
            <p>But here's the exciting part: can we escape this curse of dimensionality by <span style="color: #FF1493;">adding more structure</span> to our problem? The answer is <span style="color: #20B2AA; font-weight: bold;">yes</span>!</p>
            <div style="border: 2px solid #20B2AA; padding: 15px; margin-bottom: 20px;">
                <p><span style="color: #20B2AA; font-size: larger; font-weight: bold;">Proposition:</span></p>
                <p>Under the assumption that there exists $\mu \in \mathscr{P}_2(\mathbb{R}^d)$ and $U^* \in O(d)$ such that $W_2^2(\mu, P_{U^*}\#\mu) \leq \epsilon$ for some fixed $\epsilon > 0$, we can prove that:</p>
                <p style="text-align: center;">$\mathbb{E}(W_2^2(\mu, \mu_n)) \leq 6\epsilon + 3d^2 n^{-1}$</p>
            </div>
            <p>Why is this result important? Our <span style="color: #FF1493; font-weight: bold;">goal</span> is to recover the cross structure from data by finding $U_n \in \text{argmin}_{U\in O(d)}W_2^2(\mu_n, P_U\#\mu_n)$. The key is to <span style="font-weight: bold;">connect</span> <span style="color: #32CD32;">dist$(U_n, U^*)$</span> with $W_2(\mu, \mu_n)$.</p>
            <p>For solutions $U_n$, we can show:</p>
            <p style="text-align: center;">
                $\mathbb{E}(\text{dist}(U_n, U^*)^2) \leq \frac{2}{C_\mu}\mathbb{E}(W_2^2(P_{U^*}\#\mu, P_{U_n}\#\mu)) \leq \frac{2}{C_\mu}(2\epsilon + \mathbb{E}(W_2^2(\mu, \mu_n)))$
            </p>
            <div style="border: 2px solid #20B2AA; padding: 15px; margin-bottom: 20px;">
                <p><span style="color: #20B2AA; font-size: larger; font-weight: bold;">Theorem</span></p>
                <p>Under the assumptions of the previous proporsition, for $U_n \in \text{argmin}_{U\in O(d)}W_2^2(\mu_n, P_U\#\mu_n)$, we get:</p>
                <p style="text-align: center;">$\mathbb{E}(\text{dist}(U_n, U^*)^2) \leq \frac{1}{C_\mu}(16\epsilon + 6d^2n^{-1})$</p>
            </div>
            <div style="display: flex; gap: 20px; align-items: center; margin: 20px 0;">
                <div style="flex: 1; display: flex; justify-content: center;">
                    <img src="images/log_error_id_WFA2.png" alt="Dimensionality Analysis" style="width: 100%; max-width: 400px;">
                </div>
                <div style="flex: 1; display: flex; flex-direction: column; align-items: flex-start;">
                    <p>Looking at the normalized error $\delta = \frac{\text{dist}^2(U_n, U^*)}{\|U^*\|^2}$, we reach an exciting conclusion: <span style="color: #32CD32; font-weight: bold;">we've avoided the curse of dimensionality!</span></p>
                </div>
            </div>
        </div>

        <div class="subsection">

<h2>Real-world Data Applications</h2>

<h3 style="color: #8A2BE2; font-size: 24px;">EOF Analysis in Climate Science</h3>

<div style="display: flex; gap: 20px; align-items: center; margin: 20px 0;">
    <!-- Left column - Text -->
    <div style="flex: 1;">
        <p>When studying Earth's climate, researchers face a challenge: tracking how <span style="font-weight: bold;">spatial patterns</span> evolve through time. This involves analyzing complex multidimensional time series.</p>
        <p>We <span style="color: #20B2AA; font-weight: bold;">aim</span> to uncover relevant factors hidden in this complexity. For this analysis, we focus on Sea Surface Temperature data, tracking where and when temperatures deviate from their normal patterns.</p>
    </div>

    <!-- Right column - Plot -->
    <div style="flex: 1; display: flex; justify-content: center;">
        <div style="position: relative;">
            <img src="images/world.png" alt="EOF Analysis" style="width: 100%; max-width: 400px;">
        </div>
    </div>
</div>
<p>To give this challenge a mathematical precision, we organize our measurements into vector fields and study their relationships through covariance:</p>
        <p style="text-align: center;">$X = [X_1,\ldots,X_d]^\top$, with $X_i = [x_{t1},\ldots,x_{td}]$ $\implies$ $\text{Cov}(X) = \mathbb{E}XX^\top$</p>
<div style="display: flex; gap: 20px; align-items: center; margin: 20px 0;">
    <!-- Left side - Two plots -->
    <div style="flex: 2; display: flex; flex-direction: row; gap: 10px; justify-content: center;">
        <div>
            <img src="images/el_nino_plot.png" alt="El Niño Pattern" style="width: 100%; max-width: 300px;">
        </div>
        <div>
            <img src="images/la_nina_plot.png" alt="La Niña Pattern" style="width: 100%; max-width: 300px;">
        </div>
    </div>
    
    <!-- Right side - Text -->
    <div style="flex: 1;">
        <p>When applied to sea surface temperatures, our method identifies the <span style="color: #FF8C00;">El Niño</span> warming patterns in the central and eastern Pacific, while orthogonality captures <span style="color: #0066CC;">La Niña</span> cool waters.</p>
    </div>
</div>

<h3 style="color: #8A2BE2; font-size: 24px;">Understanding Human Faces</h3>

<div style="display: flex; gap: 40px; justify-content: center; margin: 20px 0;">
    <div style="display: flex; flex-direction: column; align-items: center;">
        <span style="color: #FF1493; font-weight: bold; font-size: 20px;">PCA</span>
        <div style="display: flex; flex-direction: column; gap: 10px;">
            <img src="images/pca_original.png" alt="PCA Original" style="width: 100%; max-width: 300px;">
            <img src="images/pca_processed.png" alt="PCA Processed" style="width: 100%; max-width: 300px;">
        </div>
    </div>

    <div style="display: flex; flex-direction: column; align-items: center;">
        <span style="color: red; font-weight: bold; font-size: 20px;">Varimax</span>
        <div style="display: flex; flex-direction: column; gap: 10px;">
            <img src="images/varimax_original.png" alt="Varimax Original" style="width: 100%; max-width: 300px;">
            <img src="images/varimax_processed.png" alt="Varimax Processed" style="width: 100%; max-width: 300px;">
        </div>
    </div>

    <div style="display: flex; flex-direction: column; align-items: center;">
        <span style="color: #20B2AA; font-weight: bold; font-size: 20px;">GFA</span>
        <div style="display: flex; flex-direction: column; gap: 10px;">
            <img src="images/gfa_original.png" alt="GFA Original" style="width: 100%; max-width: 300px;">
            <img src="images/gfa_processed.png" alt="GFA Processed" style="width: 100%; max-width: 300px;">
        </div>
    </div>
</div>

<p style="text-align: left; margin-top: 20px;">
    Looking at the results for the Faces Dataset, GFA provides a <span style="font-weight: bold;">more interpretable</span> facial features compared to traditional methods like Varimax.
</p>
        </div>

        <div class="subsection">
            <h2>References</h2>
            
                Diaz, M., García-Trillos, N., Lopez-Castaño, D., & Yang, C. (2024). Geometric Factor Analysis. <em>In preparation</em>.<br></br>
                
                Rohe, K., & Zeng, M. (2023). Vintage factor analysis with Varimax performs statistical inference. <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, 85(4), 1019-1043.
            
         </div>
    </div>

    <footer>
        <p>&copy; 2024 Daniel López-Castaño. All rights reserved.</p>
    </footer>
</body>
</html>